tokenizer:
  name: "openai-community/gpt2"
  special_tokens:
    pad_token: "[PAD]"
    eos_token: "[EOS]"
    bos_token: "[BOS]"
  post_processor:
    template: "{bos} $A {eos}"

datasets:
  unified:
    module: "datasets_def"
    class: "UnifiedDataset"
    data_loader:
      module: "utils"
      function: "load_and_concatenate_dino_data"
      args: []
    text_data_loader:
      module: "utils"
      function: "load_and_concatenate_text_only_data"
      args:
        - "./data/text_only/train_50M"

dataloaders:
  batch_size: 64
  num_workers: 4
  splits:
    train: 0.8
    val: 0.1
    test: 0.1
  indices_file: "./data/unified_indices"
  save_indices: true

model:
  module: "model"
  class: "DualStreamTransformer"
  params:
    d_model: 768
    n_head: 8
    d_hid: 3072
    num_encoder_layers: 5
    num_decoder_layers: 8
    dino_dim: 768
    dropout: 0.1

trainer:
  module: "trainer"
  class: "Trainer"
  params:
    lr: 5e-5
    weight_decay: 0.01
    total_steps: 1107020
    text_only_epochs: 5
    image_caption_epochs: 5
    clip_grad_norm: 1.0
    eval_steps: 50000
    checkpoint_steps: 50000
    warmup_steps: 12000
    wandb_project: "dual-stream-model"
    unified: true
  
  dataloader_mapping:
    unified_train_loader: "unified_train"
    unified_val_loader: "unified_val"
    unified_test_loader: "unified_test"

training:
  device: "cuda"
  seed: 42
  deterministic: false
  max_epochs: 10
  checkpoint_dir: "./checkpoints/medium_size"
  run_name: "medium_size"
  resume_from: null